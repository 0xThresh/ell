===========
Message API
===========

Messages are the fundamental unit of interacting with chat based language models. Period. they consist of a role Some sort of content period. Typically with text only language models, the content is just a string, but with multimodal language models that can process images, audio Text and other modalities. The content object is less clear. Period. In practice. The contents that a lineage model can consume actually forms a Markup language comma where There are different content blocks of text, images, audio, tool use and so on. Period. 

As a result of The potential complexity of a message object. Language model apis Have established A message specification that is oftentimes quite pedantic when the user may only want to pass around Simple types like strings or images. Adding to this issue is the fact that most language model Apis Are actually automatically generated via? stainless Cogeneration tool that takes an api Spec and builds Multiport Client side api bindings across many different languages. For that spec. Because the majority of the LM API's. Are automatically generated? They They can't be optimized for. User-friendlieness.

For example, a large majority Of. Prompt engineering libraries exist to solve. The inconvenience of indexing into responses from the open AI API period. 

```
result : str = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What is the capital of the moon?"}
    ]
)["choices"][0]["message"]["content"] # hughkguht this line

result : str my_prompt_engineering_library("prompt")
```
Likewise Specification of prompts themselves It is also quite cumbersome. Because no developer experienced tooling can be implemented. Within language model provider api client signed bindings and so. their users Need to be as verbose empedantic as possible. consider Passing. An input with text and images to a language model. Api 


```
result : str = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": [ # highlight these lines 
            {"type": "text", "text": "What is the capital of the moon?"},
            {"type": "image_url", "image_url": {"url": "https://example.com/image.jpg"}} 
        ]}
    ]
)["choices"][0]["message"]["content"] 
```
In essence, the user has to specify explicitly two different content blocks and their type, even though their type is implicit and can be inferred just because the language bindings are using typed dictionaries and identic validators generated by Stainless or some other co-generation tool Period. This is not inherently wrong, but it leaves a gap in developer experience that Prompt engineering. Code very much unreadable 

This leads us to a core philosophy in L. .

> Using language models is just passing around strings except when it';s not.


## Solution 




.. code-block:: python

   from ell import Message, system, user, assistant

   messages = [
       system("You are a helpful assistant."),
       user("Hello, how are you?"),
       assistant("I'm doing well, thank you! How can I assist you today?")
   ]