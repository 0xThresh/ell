from typing import Optional
from pydantic import BaseModel, Field
import ell
import requests

import ell.decorators.tool
from ell.lstr import lstr
from ell.stores.sql import SQLiteStore


@ell.tool()
def get_html_content(
    url : str = Field(description="The URL to get the HTML content of. Never incldue the protocol (like http:// or https://)"),
    ):
    """Get the HTML content of a URL."""
    return lstr(requests.get(url))


@ell.lm(model="gpt-4o", tools=[get_html_content], eager=True)
def summarize_website(website :str) -> str:
    """You are an agent that can summarize the contents of a website."""
    return f"Tell me whats on {website}"


if __name__ == "__main__":
    ell.config.verbose = True
    ell.set_store(SQLiteStore("sqlite_example"), autocommit=True)
    summarize_website("nyt front page")
    # Options for behaviour:
    # 1. this will call the tool and return the result if the tool is called
    # 2. This will return a partial of get_html_content that you call .
    #     ie. x = summarize_website("nyt front page") -
    #         x() calls the tools with the arguments
    # 3. This will return a tuple of the function to call and the arguemnts
    #          x = summarize_website("nyt front page")
    #          x[0](x[1])
    # 4. This will return the object form of the OpenAI resposne:
    #          x = summarize_website("nyt front page")
    #          x.tool_calls[0].function.arguments
    # 5. We make some really nice mixed message thign:
    #          x = summarize_website("nyt front page")
    #          [lstr(content: ''), lcall(function=get_html_content, arguments=<whatever from the model>)]
    #          x[1]()
    # 6. Your ideas here!


"""
todo:
Turn on Structured Outputs by setting strict: "true"
When Structured Outputs is turned on, the arguments generated by the model for function calls will reliably match the JSON Schema that you provide.

If you are not using Structured Outputs, then the structure of arguments is not guaranteed to be correct, so we recommend the use of a validation library like Pydantic to first verify the arguments prior to using them.

todo: eager execution of tools & responses to models
unified tool api for both anthropci & openai

todo:
chat mode/threads & threading.

todo:
unified output format

todo: 
definitional clarity around what non eager mode looks like.



"""




with ell.thread(model="gpt-4o", tools=[get_html_content], eager=True) as thread:
    thread.send(content="Hello! Can you help me summarize the front page of the New York Times?", role="user")
    response = thread.receive()
    print("Assistant:", response.content)

    thread.send(content="Great! Please proceed with summarizing the NYT front page.", role="user")
    response = thread.receive()
    print("Assistant:", response.content)

    thread.send(content="That's interesting. Can you highlight the main headline?", role="user")
    response = thread.receive()
    print("Assistant:", response.content)

    thread.send(content="Thank you. Lastly, are there any significant world news stories?", role="user")
    response = thread.receive()
    print("Assistant:", response.content)


@ell.thread(model="gpt-4o", tools=[get_html_content])
def conversation():
    ell.send(content="Hello! Can you help me summarize the front page of the New York Times?", role="user")
    response = ell.receive()
    print("Assistant:", response.content)

    ell.send(content="Great! Please proceed with summarizing the NYT front page.", role="user")
    response = ell.receive()
    print("Assistant:", response.content)

    ell.send(content="That's interesting. Can you highlight the main headline?", role="user")
    response = ell.receive()
    print("Assistant:", response.content)

    ell.send(content="Thank you. Lastly, are there any significant world news stories?", role="user")
    response = ell.receive()
    print("Assistant:", response.content)


class Person(BaseModel):
    personality : str
    name :str
    chat_history : ell.Thread

    def act(self, state):
        return f"{self.name} is a {self.personality} person."
    
    def chat_with_(self, state, other_person, msg):
        with ell.chat(thread=self.history, model="gpt-4o", tools=[get_html_content]) as (send, receive):
            send(content=msg, role="user")
            response = receive()
            return response.content 



class Person(BaseModel):
    personality : str
    name :str

    def act(self, state):
        return f"{self.name} is a {self.personality} person."
    
    @ell.chat(model="gpt-4o", tools=[get_html_content])
    def chat_with_(self, state, other_person, msg):
        thread = ell.get_thread()
        thread.send(content=msg, role="user")
        response = thread.receive()
        return response.content 




thread = ell.Thread()
# Must be used in a an ell.chat context.
with ell.chat(thread=thread, model="gpt-4o", tools=[get_html_content]):
    thread.send(content="Hello! Can you help me summarize the front page of the New York Times?", role="user")
    response = thread.receive()
    print("Assistant:", response.content)


def l(*args, **kwargs):
    pass

# Content markup language.
thread.send(content=l(
    l("Hello! Can you help me summarize the front page of the New York Times?"),
    l(image="https://example.com/image.jpg")
    l(tool_result=get_html_content("https://example.com"))
))


###########################

# This is a markup language

message = Union[list[
    text,
    fn call,
    text,
    image
], text]...

# Now do we ask for that or have to chck every time?
# Well in general we have to check every time...


# so

@ell.lm -> only produces text

@ell.agent -> produces an a message type?

@ell.multimodal -> produces a message type?

@ell.structured -> (model=, schema=<>)