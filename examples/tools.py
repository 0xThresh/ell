from typing import Optional
from pydantic import BaseModel, Field
import ell
import requests

import ell.lmp.tool
from ell.lstr import lstr
from ell.stores.sql import SQLiteStore


@ell.tool()
def get_html_content(
    url : str = Field(description="The URL to get the HTML content of. Never incldue the protocol (like http:// or https://)"),
    ):
    """Get the HTML content of a URL."""
    return lstr(requests.get(url))


@ell.text(model="gpt-4o", tools=[get_html_content], eager=True)
def summarize_website(website :str) -> str:
    """You are an agent that can summarize the contents of a website."""
    return f"Tell me whats on {website}"


if __name__ == "__main__":
    ell.config.verbose = True
    ell.set_store(SQLiteStore("sqlite_example"), autocommit=True)
    summarize_website("nyt front page")





    # Options for behaviour:
    # 1. this will call the tool and return the result if the tool is called
    # 2. This will return a partial of get_html_content that you call .
    #     ie. x = summarize_website("nyt front page") -
    #         x() calls the tools with the arguments
    # 3. This will return a tuple of the function to call and the arguemnts
    #          x = summarize_website("nyt front page")
    #          x[0](x[1])
    # 4. This will return the object form of the OpenAI resposne:
    #          x = summarize_website("nyt front page")
    #          x.tool_calls[0].function.arguments
    # 5. We make some really nice mixed message thign:
    #          x = summarize_website("nyt front page")
    #          [lstr(content: ''), lcall(function=get_html_content, arguments=<whatever from the model>)]
    #          x[1]()
    # 6. Your ideas here!


"""
todo:
Turn on Structured Outputs by setting strict: "true"
When Structured Outputs is turned on, the arguments generated by the model for function calls will reliably match the JSON Schema that you provide.

If you are not using Structured Outputs, then the structure of arguments is not guaranteed to be correct, so we recommend the use of a validation library like Pydantic to first verify the arguments prior to using them.

todo: eager execution of tools & responses to models
unified tool api for both anthropci & openai

todo:
chat mode/threads & threading.

todo:
unified output format

todo: 
definitional clarity around what non eager mode looks like.



"""




with ell.thread(model="gpt-4o", tools=[get_html_content], eager=True) as thread:
    thread.send(content="Hello! Can you help me summarize the front page of the New York Times?", role="user")
    response = thread.receive()
    print("Assistant:", response.content)

    thread.send(content="Great! Please proceed with summarizing the NYT front page.", role="user")
    response = thread.receive()
    print("Assistant:", response.content)

    thread.send(content="That's interesting. Can you highlight the main headline?", role="user")
    response = thread.receive()
    print("Assistant:", response.content)

    thread.send(content="Thank you. Lastly, are there any significant world news stories?", role="user")
    response = thread.receive()
    print("Assistant:", response.content)


@ell.thread(model="gpt-4o", tools=[get_html_content])
def conversation():
    ell.send(content="Hello! Can you help me summarize the front page of the New York Times?", role="user")
    response = ell.receive()
    print("Assistant:", response.content)

    ell.send(content="Great! Please proceed with summarizing the NYT front page.", role="user")
    response = ell.receive()
    print("Assistant:", response.content)

    ell.send(content="That's interesting. Can you highlight the main headline?", role="user")
    response = ell.receive()
    print("Assistant:", response.content)

    ell.send(content="Thank you. Lastly, are there any significant world news stories?", role="user")
    response = ell.receive()
    print("Assistant:", response.content)


class Person(BaseModel):
    personality : str
    name :str
    chat_history : ell.Thread

    def act(self, state):
        return f"{self.name} is a {self.personality} person."
    
    def chat_with_(self, state, other_person, msg):
        with ell.chat(thread=self.history, model="gpt-4o", tools=[get_html_content]) as (send, receive):
            send(content=msg, role="user")
            response = receive()
            return response.content 



class Person(BaseModel):
    personality : str
    name :str

    def act(self, state):
        return f"{self.name} is a {self.personality} person."
    
    @ell.chat(model="gpt-4o", tools=[get_html_content])
    def chat_with_(self, state, other_person, msg):
        thread = ell.get_thread()
        thread.send(content=msg, role="user")
        response = thread.receive()
        return response.content 




thread = ell.Thread()
# Must be used in a an ell.chat context.
with ell.chat(thread=thread, model="gpt-4o", tools=[get_html_content]):
    thread.send(content="Hello! Can you help me summarize the front page of the New York Times?", role="user")
    response = thread.receive()
    print("Assistant:", response.content)


def l(*args, **kwargs):
    pass

# Content markup language.
thread.send(content=l(
    l("Hello! Can you help me summarize the front page of the New York Times?"),
    l(image="https://example.com/image.jpg")
    l(tool_result=get_html_content("https://example.com"))
))


###########################

# This is a markup language

message = Union[list[
    text,
    fn call,
    text,
    image
], text]...

# Now do we ask for that or have to chck every time?
# Well in general we have to check every time...


# so

@ell.text -> only produces text

@ell.agent -> produces an a message type?

@ell.multimodal -> produces a message type?

@ell.structured -> (model=, schema=<>)


@ell.text
-> produces openai message type

[
    {
        type: "text",
        content: "Hello! Can you help me summarize the front page of the New York Times?"
    },
    {
        type: "tool_call",
        call: fn,
        function: {
            name: "get_html_content",
            arguments: "https://example.com"
        }
    }
]


or 
@ell.text 
-> produces 
    ("Hello! Can you help me summarize the front page of the New York Times?",
    )

msg_part = Union[lstr, InstanceOf[ell.BaseModel], fn_str]
msg = Union[msg_part, list[msg_part]]

# The LM can produce many types of outputs. 
# you could force this bitch to be type safe on the other side?

@ell.completion -> produces a text completion lstr?

@ell.chat
-> produces markup


x = do_something(msg) 
# This is the code you need to do evey single time? Well it's random though on the otherside. So that's why langchain did their |OutputParsers bullshit
if isinstance(x, lstr):
    x = x.content
elif isinstance(x, ell.BaseModel):
    x = x.model_dump_json()
elif isinstance(x, fn):
    x = x()


# @ell.lm(model="gpt-4o", tools=[get_html_content], tool_choice="auto")
# def do_something(msg):
#     return "prompt"

# while True:
#     y = input()

#     try: 
#         out = do_something(y) -> lstr | ell.StructuredOutput | ell.ToolCall
#         if out.is_str():
#             print(out)
#         elif out.is_tool_call():
#             print(out())
#         elif out.is_structured():
#             print(out.model_dump_json())
        
#     except ell.Refusal as e:
#         print(e)




@ell.text(model="gpt-4o", tools=[get_html_content], tool_choice="auto")
def my_prompt(msg):
    return "Write a poem"


x = my_prompt("Hello!") # -> "A poem about the ocean: Turning and turning in the widening gyre"

# -> MessagePart

print(x.text)
for part in x:
    if part.text:
        # doing somethign with the text
    pass
    if part.image:
        # doing somethign with the image
        pass
    if part.tool_result:
        # doing somethign with the tool result
        pass
    if part.function_call:
        # doing somethign with the function call
        part.function_call.call()
        pass
    pass


class MessagePart(BaseModel):
    text: str
    image: str
    tool_result: str
    audio: str
    function_call  : FunctionCall

class FunctionCall(BaseModel):
    name: str
    arguments: BaseModel
    fn : Callable
    def call(self):
        return self.fn(self.arguments)
